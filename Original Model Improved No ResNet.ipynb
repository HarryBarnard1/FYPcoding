{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#See README file to see where code is sourced from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K\n",
    "import keras\n",
    "from keras.models import Sequential, Model,load_model\n",
    "#from keras.optimizers import SGD\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D,MaxPool2D, Dropout\n",
    "from keras.preprocessing import image\n",
    "from keras.initializers import glorot_uniform\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import scipy\n",
    "import random\n",
    "import gc\n",
    "import glob\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in CSV file for FER2013 data to be manipulated.\n",
    "data = pd.read_csv('/com.docker.devenvironments.code/fer2013/icml_face_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define which CSV data is image data and which is label data.\n",
    "pixel_data = data[' pixels']\n",
    "label_data = data['emotion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print number of images that can be generated from CSV data. \n",
    "len(label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-process the CSV data and reshape it into a 48x48 image.\n",
    "def preprocess_pixels(pixel_data):\n",
    "  images = []\n",
    "  for i in range(len(pixel_data)):\n",
    "    img = np.fromstring(pixel_data[i], dtype='int', sep=' ')\n",
    "    img = img.reshape(48,48,1)\n",
    "    images.append(img)\n",
    "\n",
    "  X = np.array(images)\n",
    "  return X\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Duplicate data for emotion classes with lower samples, 'auto' means all will have the same number of examples.\n",
    "oversampler = RandomOverSampler(sampling_strategy='auto')\n",
    "\n",
    "X_over, Y_over = oversampler.fit_resample(pixel_data.values.reshape(-1,1), label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test that the reshaped data fits a 48x48 image.\n",
    "X_over_series = pd.Series(X_over.flatten())\n",
    "X_over_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess data and turn into image form.\n",
    "X = preprocess_pixels(X_over_series)\n",
    "Y = Y_over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = Y_over.values.reshape(Y.shape[0],1)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.1, random_state = 45)\n",
    "print(X_train.shape, Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test random image from dataset and display it to check that it has formed an image.\n",
    "plt.imshow(X[25000,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define new model.\n",
    "def emotion_recognition(input_shape):\n",
    "\n",
    "  X_input = Input(input_shape)\n",
    "\n",
    "  X = Conv2D(32, kernel_size=(3,3), strides=(1,1), padding='same')(X_input)\n",
    "  X = BatchNormalization(axis=3)(X)\n",
    "  X = Activation('relu')(X)\n",
    "\n",
    "\n",
    "  X = Conv2D(32, (3,3), strides=(1,1), padding = 'same')(X)\n",
    "  X = BatchNormalization(axis=3)(X)\n",
    "  X = Activation('relu')(X)\n",
    "\n",
    "  X = MaxPooling2D((2,2))(X)\n",
    "  X = Dropout(0.2)(X)\n",
    "\n",
    "  X = Conv2D(64, (3,3), strides=(1,1), padding = 'same')(X)\n",
    "  X = BatchNormalization(axis=3)(X)\n",
    "  X = Activation('relu')(X)\n",
    "\n",
    "  X = Conv2D(64, (3,3), strides=(1,1), padding = 'same')(X)\n",
    "  X = BatchNormalization(axis=3)(X)\n",
    "  X = Activation('relu')(X)\n",
    "\n",
    "\n",
    "  X = MaxPooling2D((2,2))(X)\n",
    "  X = Dropout(0.3)(X)\n",
    "\n",
    "  X = Conv2D(128, (3,3), strides=(1,1), padding = 'valid')(X)\n",
    "  X = BatchNormalization(axis=3)(X)\n",
    "  X = Activation('relu')(X)\n",
    "\n",
    "  X = Conv2D(128, (3,3), strides=(1,1), padding = 'valid')(X)\n",
    "  X = BatchNormalization(axis=3)(X)\n",
    "  X = Activation('relu')(X)\n",
    "  \n",
    "  X = MaxPooling2D((2,2))(X)\n",
    "  X = Dropout(0.4)(X)\n",
    "\n",
    "  X = Flatten()(X)\n",
    "  X = Dense(7, activation = 'softmax')(X)\n",
    "  \n",
    "  model = Model(inputs=X_input, outputs=X)\n",
    "  \n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load weights from training, define learning rate and compile model.\n",
    "model = emotion_recognition((48,48,1))\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "#Summary of model's layers and parameters.\n",
    "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hot encode the emotion labels before training.\n",
    "y_train = to_categorical(Y_train, num_classes=7)\n",
    "y_test = to_categorical(Y_test, num_classes=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define what each result from the prediction function means for each emotion.\n",
    "label_dict = {0 : 'Angry', 1 : 'Disgust', 2 : 'Fear', 3 : 'Happiness', 4 : 'Sad', 5 : 'Surprise', 6 : 'Neutral'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction function, returns the vector of image label, as well as index of image location.\n",
    "preds = model.predict(X_train)\n",
    "def get_class(preds):\n",
    "  pred_class = np.zeros((preds.shape[0],1))\n",
    "\n",
    "  for i in range(len(preds)):\n",
    "   pred_class[i] = np.argmax(preds[i])\n",
    "\n",
    "  return pred_class\n",
    "\n",
    "pred_class_train = get_class(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN AGAIN AS SAVED TRAINING CHECKPOINT IS LOADED\n",
    "# Train model using csv data.\n",
    "#checkpoint_path = \"/com.docker.devenvironments.code/Checkpoints/cp.ckpt\"\n",
    "#checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "#cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True, verbose=1)\n",
    "\n",
    "#Model_Results = model.fit(X_train, y_train, epochs = 30, batch_size=64, validation_data=(X_test,y_test))\n",
    "#model.save('Original Improved NO RESNET')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load model to be tested.\n",
    "model = load_model('Original Improved NO RESNET')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pick a random image from the RandomImages folder, containing all test images from FER2013, not from CSV data.\n",
    "test_image_count = len(os.listdir('/com.docker.devenvironments.code/fer2013/RandomImages'))\n",
    "print('There are ' + str(test_image_count) + ' images in the testing folder')\n",
    "\n",
    "test_image = random.choice(os.listdir('/com.docker.devenvironments.code/fer2013/RandomImages'))\n",
    "\n",
    "#Load the selected image\n",
    "img_path = ('/com.docker.devenvironments.code/fer2013/RandomImages/' + test_image)\n",
    "img = image.load_img(img_path, grayscale=True, target_size=(48,48,1))\n",
    "\n",
    "#Translate image into array form so it can be fed into the prediction function.\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "\n",
    "#Display the image and the predicted emotion as an output.\n",
    "\n",
    "prediction = np.argmax(model.predict(x))\n",
    "print('The predicted emotion is : ' + label_dict[prediction])\n",
    "my_image = image.load_img(img_path)\n",
    "plt.imshow(my_image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate on Test Data:\n",
    "results = model.evaluate(X_test, y_test, batch_size = 256)\n",
    "print(\"test loss, test acc\",results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.subplot(2,1,1)\n",
    "#plt.plot(Model_Results.history['accuracy'])\n",
    "#plt.plot(Model_Results.history['val_accuracy'])\n",
    "#plt.title('Original Model Improved - Accuracy')\n",
    "#plt.ylabel('accuracy')\n",
    "#plt.xlabel('epoch')\n",
    "#plt.legend(['train', 'test'], loc = 'lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.subplot(2,1,2)\n",
    "#plt.plot(Model_Results.history['loss'])\n",
    "#plt.plot(Model_Results.history['val_loss'])\n",
    "#plt.title('Original Model Improved- Loss')\n",
    "#plt.ylabel('loss')\n",
    "#plt.xlabel('epoch')\n",
    "#plt.legend(['train', 'test'], loc = 'upper right')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
